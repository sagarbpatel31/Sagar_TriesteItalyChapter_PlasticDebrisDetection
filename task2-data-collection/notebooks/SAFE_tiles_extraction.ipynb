{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a016576",
   "metadata": {},
   "source": [
    "#  Download full SAFE archive as .zip and upload to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d25aba",
   "metadata": {},
   "source": [
    " __To use the features in this notebook you need to visit https://dataspace.copernicus.eu and create an account with Copernicus, the official governing body of Sentinel Missions for the European Space Agency (ESA). This takes about 5 minutes to do.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b61af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b6d8c",
   "metadata": {},
   "source": [
    "# Batch setup\n",
    "Steps:\n",
    "- make sure your credentials you use to log into are appropriately stored in a .env file in this format with quotation marks: <br>\n",
    "CDSE_email = 'youremail' <br>\n",
    "CDSE_password = 'yourpassword'<br><br>\n",
    "- change satellite, if S2B batch <br><br>\n",
    "- change startDate and endDate to reflect the Quarter for your batch\n",
    "<br>i.e. all days inclusive | Q1: Jan-Mar | Q2: Apr-Jun | Q3: Jul-Sep | Q4: Oct-Dec<br><br>\n",
    "- output_dir to reflect the REgion & time for your batch in folder name<br> <br>__!!KEEP it strictly in RE_MMYY format!!__  <br><br>\n",
    "- leave query_satellite and query_tile unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e904c",
   "metadata": {},
   "source": [
    "\n",
    "### üõ∞Ô∏è Sentinel-2 Data Summary\n",
    "\n",
    "| üõ∞Ô∏è Satellite Type | üìÖ From       | üìÖ To         | üì¶ Number of SAFE Files | üíæ Estimated Size     |\n",
    "|-------------------|--------------|--------------|--------------------------|------------------------|\n",
    "| S2A_MSIL1C         | 2015-07-04   | 2016-10-16   | 30                       | Maximum 27 GB üíΩ       |\n",
    "| S2A_MSIL1C         | 2017-02-20   | 2017-10-08   | 17                       | Maximum 17 GB üíΩ       |\n",
    "| S2A_MSIL1C         | 2018-03-27   | 2018-11-12   | 27                       | Maximum 24 GB üíΩ       |\n",
    "| <s>S2A_MSIL1C         | <s>2019-02-13   | <s>2019-10-11   | <s>17</s> 19                       | Maximum 15 GB üíΩ     |\n",
    "| S2A_MSIL1C         | 2020-03-16   | 2020-11-04   | 18                       | Maximum 16 GB üíΩ       |\n",
    "| S2A_MSIL1C         | 2021-03-01   | 2021-08-18   | 18                       | Maximum 16 GB üíΩ       |\n",
    "| S2B_MSIL1C         | 2017-07-05   | 2018-11-17   | 37                       | Maximum 32 GB üíΩ       |\n",
    "| S2B_MSIL1C         | 2019-02-08   | 2019-10-16   | 14                       | Maximum 13 GB üíΩ       |\n",
    "| S2B_MSIL1C         | 2020-02-20   | 2020-11-19   | 21                       | Maximum 19 GB üíΩ       |\n",
    "| S2B_MSIL1C         | 2021-03-29   | 2021-06-17   | 10                       | Maximum 8 GB üíΩ        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b61131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Required satellite category\n",
    "query_satellite = 'SENTINEL-2'\n",
    "\n",
    "# 2 String to be included in filename for retrieval of specific product by name, \n",
    "# i.e S2A vs S2B, and code for tile name\n",
    "query_product = 'S2A_MSIL1C_' # change to S2B_MSIL1C_\n",
    "query_tile = 'T33TUL'   # stays the same\n",
    "\n",
    "# 3 Enter a start and end date\n",
    "query_startDate = '2019-01-01'\n",
    "query_endDate = '2019-12-31'\n",
    "\n",
    "# 4 Load your credentials from .env\n",
    "load_dotenv()\n",
    "username=os.getenv(\"CDSE_email\")\n",
    "password=os.getenv(\"CDSE_password\")\n",
    "# if not already in .env config, insert them as 'string' \n",
    "# values in the following format to the .env file:\n",
    "CDSE_email = username\n",
    "CDSE_password = password\n",
    "\n",
    "# 5 Set output file:\n",
    "output_dir = './SAFE/PO_2A_19' #edit folder name within SAFE/ as appropriate to add batch folders\n",
    "# i.e. keep format like \n",
    "# ./SAFE/PO_2A_19 for S2A 2019 (Apr-Jun) \n",
    "# ./SAFE/PO_2B_21 for S2B 2021 (Jan-Mar) S2B etc. \n",
    "# ! DO NOT CHANGE THE LENGTH OF THE FOLDER NAME! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c45810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\":username,\n",
    "        \"password\":password,\n",
    "        \"grant_type\": \"password\",\n",
    "        }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "            )\n",
    "    print(\"Access token created successfully!\")\n",
    "    return r.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b281cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_https_request(satellite, product, tile, start_date, end_date): #, geojson\n",
    "    \n",
    "    base_prefix = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=\"\n",
    "    collection = \"Collection/Name eq '\" + satellite + \"' and startswith(Name,'\" + product + \"') and contains(Name,'\" + tile + \"')\"\n",
    "    #roi_coordinates = get_coordinates(geojson)\n",
    "    #geographic_criteria = \"OData.CSC.Intersects(area=geography'SRID=4326;POLYGON((\" + roi_coordinates + \"))') \"\n",
    "    content_date = (\n",
    "            \"ContentDate/Start gt \" + start_date + \"T00:00:00.000Z and \" +\n",
    "            \"ContentDate/Start lt \" + end_date + \"T00:00:00.000Z\"\n",
    "    )\n",
    "    https_request = ( base_prefix + collection +  \" and \" #Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq 'cloudCover' and att/OData.CSC.DoubleAttribute/Value le 20.00) and \" \n",
    "                     + content_date) # geographic_criteria + \" and \" +\n",
    "    print(\"Query URL:\", https_request)\n",
    "    return https_request\n",
    "\n",
    "\n",
    "def download_data(token, id, name, length, output):\n",
    "    url = f\"https://download.dataspace.copernicus.eu/odata/v1/Products({id})/$value\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    response = session.get(url, headers=headers, stream=True)\n",
    "    try:\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Downloading: '+name)\n",
    "        with open(output, \"wb\") as file:\n",
    "            if length is not None:\n",
    "                # set the total length of the progress bar for tracking downloads\n",
    "                pbar = tqdm(total=length, unit=\"B\", unit_scale=True, desc=name)\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        # update progress bar\n",
    "                        pbar.update(len(chunk))\n",
    "                pbar.close()\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Download complete: '+name)\n",
    "        response.close()\n",
    "    except Exception as e:\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Download failed: '+name)\n",
    "        print(f\"An exception occured: {e}\")\n",
    "\n",
    "\n",
    "# zip the Safe files for download\n",
    "def get_file_name(name):\n",
    "    file_name = ''\n",
    "    if query_satellite == 'SENTINEL-1':\n",
    "        file_name = name.replace(\".SAFE\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-2':\n",
    "        file_name = name.replace(\".SAFE\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-3':\n",
    "        file_name = name.replace(\".SEN3\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-5P':\n",
    "        file_name = name.replace(\".nc\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-6':\n",
    "        file_name = name.replace(\".SEN6\", \".zip\")\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e7617",
   "metadata": {},
   "source": [
    "# Download non-duplicate tiles matched to litter row data. \n",
    "Make sure the path  for ```litterrows = pd.read_excel('../files/LM_centroids.xlsx')```is reflected in your folder structure or \n",
    "if using colab, changed to './LM_centroids.xlsx' and the file added to content folder (current workign dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3610dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query URL: https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2' and startswith(Name,'S2A_MSIL1C_') and contains(Name,'T33TUL') and ContentDate/Start gt 2019-01-01T00:00:00.000Z and ContentDate/Start lt 2019-12-31T00:00:00.000Z\n",
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.SAFE\n",
      "Access token created successfully!\n",
      "[ 02:56:14 ] Downloading: S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18.2M/18.2M [00:02<00:00, 6.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 02:56:17 ] Download complete: S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.zip\n",
      "S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.SAFE\n",
      "Access token created successfully!\n",
      "[ 02:56:19 ] Downloading: S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670M/670M [01:50<00:00, 6.06MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 02:58:09 ] Download complete: S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.zip\n",
      "S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.SAFE\n",
      "Access token created successfully!\n",
      "[ 02:58:11 ] Downloading: S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 627M/627M [02:37<00:00, 3.98MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:00:49 ] Download complete: S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip\n",
      "S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.SAFE\n",
      "Access token created successfully!\n",
      "[ 03:00:51 ] Downloading: S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 704M/704M [01:43<00:00, 6.82MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:02:34 ] Download complete: S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip\n",
      "S2A_MSIL1C_20190111T100401_N0500_R122_T33TUL_20221208T040313.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190111T100401_N0500_R122_T33TUL_20221208T040313.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.SAFE\n",
      "Access token created successfully!\n",
      "[ 03:02:36 ] Downloading: S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 710M/710M [01:47<00:00, 6.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:04:24 ] Download complete: S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.zip\n",
      "S2A_MSIL1C_20190121T100321_N0500_R122_T33TUL_20221202T130949.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190121T100321_N0500_R122_T33TUL_20221202T130949.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190124T101311_N0500_R022_T33TUL_20221214T111444.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190124T101311_N0500_R022_T33TUL_20221214T111444.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190131T100241_N0500_R122_T33TUL_20221209T001656.SAFE\n",
      "Access token created successfully!\n",
      "[ 03:04:26 ] Downloading: S2A_MSIL1C_20190131T100241_N0500_R122_T33TUL_20221209T001656.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190131T100241_N0500_R122_T33TUL_20221209T001656.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 783M/783M [01:57<00:00, 6.65MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:06:24 ] Download complete: S2A_MSIL1C_20190131T100241_N0500_R122_T33TUL_20221209T001656.zip\n",
      "S2A_MSIL1C_20190203T101221_N0500_R022_T33TUL_20221206T131136.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190203T101221_N0500_R022_T33TUL_20221206T131136.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190210T100141_N0500_R122_T33TUL_20221127T074340.SAFE\n",
      "Access token created successfully!\n",
      "[ 03:06:26 ] Downloading: S2A_MSIL1C_20190210T100141_N0500_R122_T33TUL_20221127T074340.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190210T100141_N0500_R122_T33TUL_20221127T074340.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 645M/645M [02:34<00:00, 4.19MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:09:01 ] Download complete: S2A_MSIL1C_20190210T100141_N0500_R122_T33TUL_20221127T074340.zip\n",
      "S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.SAFE\n",
      "Access token created successfully!\n",
      "[ 03:09:02 ] Downloading: S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 642M/642M [01:32<00:00, 6.95MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:10:35 ] Download complete: S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip\n",
      "S2A_MSIL1C_20190114T101351_N0500_R022_T33TUL_20230912T101753.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190114T101351_N0500_R022_T33TUL_20230912T101753.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.SAFE\n",
      "Access token created successfully!\n",
      "[ 03:10:37 ] Downloading: S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 786M/786M [02:10<00:00, 6.00MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:12:48 ] Download complete: S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.zip\n",
      "S2A_MSIL1C_20190223T101021_N0500_R022_T33TUL_20221127T011157.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190223T101021_N0500_R022_T33TUL_20221127T011157.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.SAFE\n",
      "Access token created successfully!\n",
      "[ 03:12:50 ] Downloading: S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 712M/712M [02:26<00:00, 4.87MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03:15:16 ] Download complete: S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.zip\n",
      "S2A_MSIL1C_20190104T101411_N0500_R022_T33TUL_20221130T134310.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190104T101411_N0500_R022_T33TUL_20221130T134310.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190305T101021_N0500_R022_T33TUL_20221117T172715.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190305T101021_N0500_R022_T33TUL_20221117T172715.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190101T100411_N0500_R122_T33TUL_20221216T215437.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190101T100411_N0500_R122_T33TUL_20221216T215437.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190521T100031_N0500_R122_T33TUL_20221215T032340.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190521T100031_N0500_R122_T33TUL_20221215T032340.zip has no recorded litter rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "request_url = get_https_request(\n",
    "    query_satellite, query_product, query_tile, query_startDate, query_endDate #, map_geojson, \n",
    ")\n",
    "JSON = requests.get(request_url).json()\n",
    "if 'detail' in JSON:\n",
    "    print(JSON['detail']['message'])\n",
    "    sys.exit()\n",
    "elif 'value' in JSON:\n",
    "    df = pd.DataFrame.from_dict(JSON['value'])\n",
    "    # print(df.columns)\n",
    "    if len(df) == 0:\n",
    "        print('No data found')\n",
    "        sys.exit()\n",
    "    \n",
    "    data_id_list = df.Id\n",
    "    data_name_list = df.Name\n",
    "    date_content_length = df.ContentLength\n",
    "else:\n",
    "    print('Unknown query error')\n",
    "    sys.exit()\n",
    "\n",
    "for i in range(len(data_id_list)):\n",
    "    print(data_name_list[i])\n",
    "    data_id = data_id_list[i]\n",
    "    data_name = get_file_name(data_name_list[i])\n",
    "    data_length = date_content_length[i]\n",
    "    # Check if the data storage path exists. If not, create the data storage path.\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_file = os.path.join(output_dir, data_name)\n",
    "    # # Check if the file has been downloaded before of it has no recorded windrows, in either case, skip it and do not download it (again).\n",
    "    litterrows = pd.read_excel('../files/LM_centroids.xlsx') # if colab : ('./LM_centroids.xlsx')\n",
    "    samples_set = set(litterrows['Str_time'])\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) == data_length:\n",
    "        print(output_file + ' File already exists')   \n",
    "    elif output_file[27:42] not in samples_set:\n",
    "        print(output_file + ' has no recorded litter rows' )\n",
    "    else:\n",
    "        access_token = get_access_token(CDSE_email, CDSE_password)\n",
    "        download_data(access_token, data_id, data_name, data_length, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c73f3",
   "metadata": {},
   "source": [
    "# kaggle upload additional batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cd876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./SAFE\n",
      "./SAFE/PO_2A_19\n",
      "./SAFE/.kaggle\n"
     ]
    }
   ],
   "source": [
    "#project_root = \"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/notebooks/SAFE\"\n",
    "project_root = \"./SAFE\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(project_root)\n",
    "\n",
    "# Path to data directory for current batch\n",
    "data_dir = project_root + output_dir[6:]\n",
    "if data_dir not in sys.path:\n",
    "    sys.path.append(data_dir)\n",
    "print(data_dir)\n",
    "\n",
    "# Set Kaggle config directory\n",
    "KAGGLE_CONFIG_DIR = os.path.expanduser(project_root + \"/.kaggle\")\n",
    "print(KAGGLE_CONFIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2237f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata file not found: dataset-metadata.json\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['kaggle', 'datasets', 'version', '-p', 'SAFE/PA_Q1_19', '-m', 'Add new zip files', '--dir-mode', 'zip']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Update the dataset on Kaggle\u001b[39;00m\n\u001b[32m     53\u001b[39m     subprocess.run([\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkaggle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m-p\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(folder_path),\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAdd new zip files\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m--dir-mode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mzip\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m     ], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mupdate_kaggle_dataset_with_zip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLitter Rows Italy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msarahajbane/litter-windrows\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mupdate_kaggle_dataset_with_zip\u001b[39m\u001b[34m(folder_path, title, dataset_id, description, license_name)\u001b[39m\n\u001b[32m     50\u001b[39m     json.dump(existing_metadata, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Update the dataset on Kaggle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkaggle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mversion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAdd new zip files\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--dir-mode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     58\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:577\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m     retcode = process.poll()\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    578\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['kaggle', 'datasets', 'version', '-p', 'SAFE/PA_Q1_19', '-m', 'Add new zip files', '--dir-mode', 'zip']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# def update_kaggle_dataset_with_zip(\n",
    "#     folder_path,\n",
    "#     title,\n",
    "#     dataset_id,\n",
    "#     description=\"Zipped Sentinel-2 L1C SAFE folders\",\n",
    "#     license_name=\"CC-BY-SA-4.0\"\n",
    "# ):\n",
    "#     folder_path = Path(folder_path)\n",
    "#     assert folder_path.exists(), \"Folder does not exist!\"\n",
    "\n",
    "#     one_back = Path('../')\n",
    "#     metadata_path = Path( one_back / \"dataset-metadata.json\")\n",
    "#     zip_files = [f.name for f in folder_path.glob(\"*.zip\")]\n",
    "\n",
    "#     # If metadata already exists - loads it\n",
    "    \n",
    "#     if metadata_path.exists():\n",
    "#         with open(metadata_path, 'r') as f:\n",
    "#             existing_metadata = json.load(f)\n",
    "#             existing_paths = {res[\"path\"] for res in existing_metadata.get(\"resources\", [])}\n",
    "#     else:\n",
    "#         existing_metadata = {\n",
    "#             \"title\": title,\n",
    "#             \"id\": dataset_id,\n",
    "#             \"licenses\": [{\n",
    "#                 \"name\": license_name,\n",
    "#                 \"title\": \"Creative Commons Attribution Share-Alike 4.0\",\n",
    "#                 \"path\": \"https://creativecommons.org/licenses/by-sa/4.0/\"\n",
    "#             }],\n",
    "#             \"resources\": []\n",
    "#         }\n",
    "#         existing_paths = set()\n",
    "\n",
    "#     # Add only new zip files to the resources list \n",
    "#     for zipf in zip_files:\n",
    "#         if zipf not in existing_paths:\n",
    "#             existing_metadata[\"resources\"].append({\n",
    "#                 \"name\": Path(zipf).stem,\n",
    "#                 \"path\": zipf,\n",
    "#                 \"description\": f\"Zipped .SAFE Sentinel-2: {zipf}\",\n",
    "#                 \"type\": \"other\",\n",
    "#                 \"format\": \"zip\"\n",
    "#             })\n",
    "\n",
    "#     with open(metadata_path, 'w') as f:\n",
    "#         json.dump(existing_metadata, f, indent=2)\n",
    "\n",
    "#     # Update the dataset on Kaggle\n",
    "#     subprocess.run([\n",
    "#         \"kaggle\", \"datasets\", \"version\",\n",
    "#         \"-p\", str(folder_path),\n",
    "#         \"-m\", \"Add new zip files\",\n",
    "#         \"--dir-mode\", \"zip\"\n",
    "#     ], check=True)\n",
    "\n",
    "# update_kaggle_dataset_with_zip(\n",
    "#     folder_path = data_dir,\n",
    "#     title=\"Litter Rows Italy\",\n",
    "#     dataset_id=\"sarahajbane/litter-windrows\",\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2834e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata file not found: dataset-metadata.json\n",
      "Error occurred while updating the Kaggle dataset: Command '['kaggle', 'datasets', 'version', '-p', 'SAFE/PA_Q1_19', '-m', 'Add new zipped .SAFE Sentinel-2 scenes', '--dir-mode', 'zip']' returned non-zero exit status 1.\n",
      "Command output: None\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['kaggle', 'datasets', 'version', '-p', 'SAFE/PA_Q1_19', '-m', 'Add new zipped .SAFE Sentinel-2 scenes', '--dir-mode', 'zip']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset updated with new files!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mupdate_kaggle_dataset_with_new_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_msg\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAdd new zipped .SAFE Sentinel-2 scenes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mupdate_kaggle_dataset_with_new_files\u001b[39m\u001b[34m(folder_path, metadata_path, update_msg)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m metadata_path.exists(), \u001b[33m\"\u001b[39m\u001b[33mMetadata does not exist!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkaggle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mversion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_msg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--dir-mode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError occurred while updating the Kaggle dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:577\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m     retcode = process.poll()\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    578\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['kaggle', 'datasets', 'version', '-p', 'SAFE/PA_Q1_19', '-m', 'Add new zipped .SAFE Sentinel-2 scenes', '--dir-mode', 'zip']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# one_back = Path('../')\n",
    "# metadata_path = Path(project_root) / \"dataset-metadata.json\"\n",
    "\n",
    "# def update_kaggle_dataset_with_new_files(folder_path, metadata_path, update_msg=\"Added new data\"):\n",
    "#     folder_path = Path(folder_path)\n",
    "#     assert folder_path.exists(), \"Folder does not exist!\"\n",
    "#     assert metadata_path.exists(), \"Metadata does not exist!\"\n",
    "#     try:\n",
    "#         subprocess.run([\n",
    "#             \"kaggle\", \"datasets\", \"version\",\n",
    "#             \"-p\", str(folder_path),\n",
    "#             \"-m\", update_msg,\n",
    "#             \"--dir-mode\", \"zip\"\n",
    "#         ], check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Error occurred while updating the Kaggle dataset: {e}\")\n",
    "#         print(f\"Command output: {e.output}\")\n",
    "#         raise\n",
    "#     print(\"Dataset updated with new files!\")\n",
    "\n",
    "# update_kaggle_dataset_with_new_files(\n",
    "#     folder_path= data_dir,\n",
    "#     metadata_path = metadata_path,\n",
    "#     update_msg=\"Add new zipped .SAFE Sentinel-2 scenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233e117",
   "metadata": {},
   "source": [
    "# create a new kaggle dataset from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: ./SAFE/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "# !kaggle datasets init -p {project_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## update metadata (template won't work) \n",
    "\n",
    "# metadata = {\n",
    "#     \"title\": \"Litter Rows Italy - Dataset For Plastic Detection Algorithms\", \n",
    "#     \"id\": \"www.kaggle.com/datasets/sarahajbane/litter-rows\", \n",
    "#       \"resources\": [\n",
    "#     {\n",
    "#       \"name\": \"Litter Rows Italy - Dataset For Plastic Detection Algorithms\",\n",
    "#       #\"path\": \"https://www.kaggle.com/datasets/sarahajbane/litter-rows\",\n",
    "#       \"description\": \"Italy Subset of Sentinel2 L1C images for litter row dataset \",\n",
    "#       \"type\": \"archive\",\n",
    "#       \"format\": \"zip\",\n",
    "#     }\n",
    "#       ],\n",
    "#     \"licenses\": [\n",
    "#         {\n",
    "#       \"name\": \"CC-BY-SA-4.0\",\n",
    "#       \"title\": \"Creative Commons Attribution Share-Alike 4.0\",\n",
    "#       \"path\": \"https://creativecommons.org/licenses/by-sa/4.0/\"\n",
    "#     } # Creative Commons license with proper attribution \n",
    "#     ] # to original authors of the litter_row dataset\n",
    "#     }\n",
    "# with open(project_root + '/' + 'dataset-metadata.json', 'w') as file:\n",
    "#     json.dump(metadata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for kaggle.json credentials\n",
    "# kaggle_json_path = os.path.expanduser(project_root + \"/.kaggle/kaggle.json\")\n",
    "# if not os.path.exists(kaggle_json_path):\n",
    "#     raise FileNotFoundError(f\"The Kaggle API credentials file is missing. Please place your kaggle.json file at {kaggle_json_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec4ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file .kaggle.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 186/186 [00:00<00:00, 468B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .kaggle.zip (186B)\n",
      "Starting upload for file .DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.00k/6.00k [00:00<00:00, 15.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .DS_Store (6KB)\n",
      "Starting upload for file PA_Q1_19.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 588M/588M [04:02<00:00, 2.54MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: PA_Q1_19.zip (588MB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/sarahajbane/litter-windrows\n"
     ]
    }
   ],
   "source": [
    "# # Function to create or update a Kaggle dataset from a local folder\n",
    "# def create_kaggle_dataset_from_folder(\n",
    "#     folder_path,\n",
    "#     title,\n",
    "#     dataset_id,\n",
    "#     description=\"Sentinel-2 L1C subset\",\n",
    "#     license_name=\"CC-BY-SA-4.0\"\n",
    "# ):\n",
    "#     folder_path = Path(folder_path)\n",
    "#     assert folder_path.exists(), \"Folder does not exist!\"\n",
    "\n",
    "#     metadata_path = folder_path / \"dataset-metadata.json\"\n",
    "#     image_files = [f.name for f in folder_path.glob(\"*.tif*\")]\n",
    "\n",
    "#     resources = [\n",
    "#         {\n",
    "#             \"name\": Path(img).stem,\n",
    "#             \"path\": img,\n",
    "#             \"description\": f\"Image: {img}\",\n",
    "#             \"type\": \"image\",\n",
    "#             \"format\": \"tiff\"\n",
    "#         } for img in image_files\n",
    "#     ]\n",
    "\n",
    "#     metadata = {\n",
    "#         \"title\": title,\n",
    "#         \"id\": dataset_id,\n",
    "#         \"licenses\": [{\n",
    "#             \"name\": license_name,\n",
    "#             \"title\": \"Creative Commons Attribution Share-Alike 4.0\",\n",
    "#             \"path\": \"https://creativecommons.org/licenses/by-sa/4.0/\"\n",
    "#         }],\n",
    "#         \"resources\": resources\n",
    "#     }\n",
    "\n",
    "#     with open(metadata_path, 'w') as f:\n",
    "#         json.dump(metadata, f, indent=2)\n",
    "\n",
    "#     # Initialize if necessary\n",
    "#     if not (folder_path / \"dataset-metadata.json\").exists():\n",
    "#         subprocess.run([\"kaggle\", \"datasets\", \"init\", \"-p\", str(folder_path)])\n",
    "\n",
    "#     # Create or version the dataset\n",
    "#     if not any((folder_path / f).exists() for f in [\"dataset-metadata.json\", \"dataset-metadata.yml\"]):\n",
    "#         print(\"No metadata found, initializing dataset.\")\n",
    "#         subprocess.run([\"kaggle\", \"datasets\", \"init\", \"-u\", str(folder_path)])\n",
    "\n",
    "#     try:\n",
    "#         subprocess.run([\n",
    "#             \"kaggle\", \"datasets\", \"create\",\n",
    "#             \"-p\", str(folder_path),\n",
    "#             \"--dir-mode\", \"zip\"\n",
    "#         ], check=True)\n",
    "#     except subprocess.CalledProcessError:\n",
    "#         subprocess.run([\n",
    "#             \"kaggle\", \"datasets\", \"version\",\n",
    "#             \"-p\", str(folder_path),\n",
    "#             \"-m\", \"Update data\",\n",
    "#             \"--dir-mode\", \"zip\"\n",
    "#         ])\n",
    "\n",
    "# # Example call to the above function\n",
    "# create_kaggle_dataset_from_folder(\n",
    "#     folder_path = project_root,\n",
    "#     title=\"Litter Rows Italy\",\n",
    "#     dataset_id=\"sarahajbane/litter-windrows\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omdena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
