{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a016576",
   "metadata": {},
   "source": [
    "#  Download full SAFE archive as .zip and upload to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d25aba",
   "metadata": {},
   "source": [
    " __To use the features in this notebook you need to visit https://dataspace.copernicus.eu and create an account with Copernicus, the official governing body of Sentinel Missions for the European Space Agency (ESA). This takes about 5 minutes to do.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b61af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b6d8c",
   "metadata": {},
   "source": [
    "# Batch setup\n",
    "Steps:\n",
    "- make sure your log in credentials for cdse are appropriately stored in the .env file in this format with quotation marks: <br><br>\n",
    "CDSE_email = 'youremail' <br>\n",
    "CDSE_password = 'yourpassword'<br><br>\n",
    "- change satellite, if S2A or S2B, depending on batch <br><br>\n",
    "- change startDate and endDate to reflect the time period for your batch\n",
    "<br><br>\n",
    "- output_dir to reflect the REgion & time for your batch in folder name<br><br>__!!Keep it strictly in PO_SX_YY format!!__ <br> longer or shorter strings will break the code <br><br>\n",
    "- leave query_satellite and query_tile unchanged\n",
    "- if you find more tiles than written in the table please update as pictured.\n",
    "- occasionally you will find duplicate records/ instances miliseconds - seconds apart (mostly with one image predominantly blank). This is usually easily noticable because one file will be very small compared to the other, but please keep them until you can unzip and verify there is no ocean visible in the scene with possible annotations. Please still record all SAFE archives downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e904c",
   "metadata": {},
   "source": [
    "\n",
    "### üõ∞Ô∏è Sentinel-2 Data Summary\n",
    "\n",
    "| üõ∞Ô∏è Satellite Type | üìÖ From       | üìÖ To         | üì¶ Number of SAFE Files | üíæ Estimated Size     |\n",
    "|-------------------|--------------|--------------|--------------------------|------------------------|\n",
    "| 1Ô∏è‚É£ - S2üÖ∞Ô∏è_MSIL1C         | 2015-07-04   | 2016-10-17   | 30                       | Maximum 27 GB üíΩ       |\n",
    "| 2Ô∏è‚É£ - S2üÖ∞Ô∏è_MSIL1C         | 2017-02-20   | 2017-10-09   | 17                       | Maximum 17 GB üíΩ       |\n",
    "| 3Ô∏è‚É£ - S2üÖ∞Ô∏è_MSIL1C         | 2018-03-27   | 2018-11-13   | 27                       | Maximum 24 GB üíΩ       |\n",
    "| 4Ô∏è‚É£ - <s>S2üÖ∞Ô∏è_MSIL1C</s> ‚úÖ        | <s>2019-02-13   | <s>2019-10-12   | <s>17</s>  *19**                       | Maximum 15 GB üíΩ     |\n",
    "| 5Ô∏è‚É£ - S2üÖ∞Ô∏è_MSIL1C         | 2020-03-16   | 2020-11-05   | 18                       | Maximum 16 GB üíΩ       |\n",
    "| 6Ô∏è‚É£ - S2üÖ∞Ô∏è_MSIL1C         | 2021-03-01   | 2021-08-19   | 18                       | Maximum 16 GB üíΩ       |\n",
    "| 7Ô∏è‚É£ - S2üÖ±Ô∏è_MSIL1C         | 2017-07-05   | 2018-11-18   | 37                       | Maximum 32 GB üíΩ       |\n",
    "| 8Ô∏è‚É£ - S2üÖ±Ô∏è_MSIL1C         | 2019-02-08   | 2019-10-17   | 14                       | Maximum 13 GB üíΩ       |\n",
    "| 9Ô∏è‚É£ - S2üÖ±Ô∏è_MSIL1C         | 2020-02-20   | 2020-11-20   | 21                       | Maximum 19 GB üíΩ       |\n",
    "| üîü - S2üÖ±Ô∏è_MSIL1C         | 2021-03-29   | 2021-06-18   | 10                       | Maximum 8 GB üíΩ        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b61131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Required satellite category\n",
    "query_satellite = 'SENTINEL-2'\n",
    "\n",
    "# 2 Strings to be included in query for retrieval of specific product by name, \n",
    "# i.e S2A vs S2B, and code for AOI tile name\n",
    "query_product = 'S2A_MSIL1C_' # change to S2B_MSIL1C_\n",
    "query_tile = 'T33TUL'   # stays the same\n",
    "\n",
    "# 3 Enter a start and end date\n",
    "query_startDate = '2019-01-01'   # change as per table above\n",
    "query_endDate = '2019-12-31'     # change as per table above\n",
    "\n",
    "# 4 Load your credentials from .env\n",
    "load_dotenv()\n",
    "username=os.getenv(\"CDSE_email\")\n",
    "password=os.getenv(\"CDSE_password\")\n",
    "# if not already in .env config, insert them as 'string' \n",
    "# values in the following format to the .env file:\n",
    "CDSE_email = username\n",
    "CDSE_password = password\n",
    "\n",
    "# 5 Set output file:\n",
    "output_dir = './SAFE/PO_2A_19' #edit folder name within SAFE/ as appropriate to add batch folders\n",
    "# i.e. keep format like: \n",
    "# ./SAFE/PO_2A_17 for Sentinel 2A until 2017\n",
    "# ./SAFE/PO_2B_18 for Sentinel 2B until 2018 etc. \n",
    "# =============================================== \n",
    "# ! DO NOT CHANGE THE LENGTH OF THE FOLDER NAME! \n",
    "# =============================================== \n",
    "# This is important for the download script to work properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36f2c0",
   "metadata": {},
   "source": [
    "## Run as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c45810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\":username,\n",
    "        \"password\":password,\n",
    "        \"grant_type\": \"password\",\n",
    "        }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "            )\n",
    "    print(\"Access token created successfully!\")\n",
    "    return r.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b281cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_https_request(satellite, product, tile, start_date, end_date):\n",
    "    \n",
    "    base_prefix = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=\"\n",
    "    collection = \"Collection/Name eq '\" + satellite + \"' and startswith(Name,'\" + product + \"') and contains(Name,'\" + tile + \"')\"\n",
    "    content_date = (\n",
    "            \"ContentDate/Start gt \" + start_date + \"T00:00:00.000Z and \" +\n",
    "            \"ContentDate/Start lt \" + end_date + \"T00:00:00.000Z\"\n",
    "    )\n",
    "    https_request = (base_prefix + collection +  \" and \" + content_date) \n",
    "    print(\"Query URL:\", https_request)\n",
    "    return https_request\n",
    "\n",
    "\n",
    "def download_data(token, id, name, length, output):\n",
    "    url = f\"https://download.dataspace.copernicus.eu/odata/v1/Products({id})/$value\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    response = session.get(url, headers=headers, stream=True)\n",
    "    try:\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Downloading: '+name)\n",
    "        with open(output, \"wb\") as file:\n",
    "            if length is not None:\n",
    "                pbar = tqdm(total=length, unit=\"B\", unit_scale=True, desc=name)\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "                pbar.close()\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Download complete: '+name)\n",
    "        response.close()\n",
    "    except Exception as e:\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Download failed: '+name)\n",
    "        print(f\"An exception occured: {e}\")\n",
    "\n",
    "\n",
    "def get_file_name(name):\n",
    "    file_name = ''\n",
    "    if query_satellite == 'SENTINEL-1':\n",
    "        file_name = name.replace(\".SAFE\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-2':\n",
    "        file_name = name.replace(\".SAFE\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-3':\n",
    "        file_name = name.replace(\".SEN3\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-5P':\n",
    "        file_name = name.replace(\".nc\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-6':\n",
    "        file_name = name.replace(\".SEN6\", \".zip\")\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e7617",
   "metadata": {},
   "source": [
    "# Download non-duplicate tiles matched to litter row data. \n",
    "Make sure the path  for ```litterrows = pd.read_excel('../files/LM_centroids.xlsx')```is reflected in your folder structure or \n",
    "if using colab, changed to './LM_centroids.xlsx' and the file added to content folder (current workign dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query URL: https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2' and startswith(Name,'S2A_MSIL1C_') and contains(Name,'T33TUL') and ContentDate/Start gt 2019-01-01T00:00:00.000Z and ContentDate/Start lt 2019-12-31T00:00:00.000Z\n",
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.zip this file already exists\n",
      "S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.zip this file already exists\n",
      "S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip this file already exists\n",
      "S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip this file already exists\n",
      "S2A_MSIL1C_20190111T100401_N0500_R122_T33TUL_20221208T040313.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190111T100401_N0500_R122_T33TUL_20221208T040313.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.zip this file already exists\n",
      "S2A_MSIL1C_20190121T100321_N0500_R122_T33TUL_20221202T130949.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190121T100321_N0500_R122_T33TUL_20221202T130949.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190124T101311_N0500_R022_T33TUL_20221214T111444.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190124T101311_N0500_R022_T33TUL_20221214T111444.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190131T100241_N0500_R122_T33TUL_20221209T001656.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190131T100241_N0500_R122_T33TUL_20221209T001656.zip this file already exists\n",
      "S2A_MSIL1C_20190203T101221_N0500_R022_T33TUL_20221206T131136.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190203T101221_N0500_R022_T33TUL_20221206T131136.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190210T100141_N0500_R122_T33TUL_20221127T074340.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190210T100141_N0500_R122_T33TUL_20221127T074340.zip this file already exists\n",
      "S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip this file already exists\n",
      "S2A_MSIL1C_20190114T101351_N0500_R022_T33TUL_20230912T101753.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190114T101351_N0500_R022_T33TUL_20230912T101753.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.zip this file already exists\n",
      "S2A_MSIL1C_20190223T101021_N0500_R022_T33TUL_20221127T011157.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190223T101021_N0500_R022_T33TUL_20221127T011157.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.zip this file already exists\n",
      "S2A_MSIL1C_20190104T101411_N0500_R022_T33TUL_20221130T134310.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190104T101411_N0500_R022_T33TUL_20221130T134310.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190305T101021_N0500_R022_T33TUL_20221117T172715.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190305T101021_N0500_R022_T33TUL_20221117T172715.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190101T100411_N0500_R122_T33TUL_20221216T215437.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190101T100411_N0500_R122_T33TUL_20221216T215437.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190521T100031_N0500_R122_T33TUL_20221215T032340.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190521T100031_N0500_R122_T33TUL_20221215T032340.zip has no recorded litter rows\n"
     ]
    }
   ],
   "source": [
    "request_url = get_https_request(\n",
    "    query_satellite, query_product, query_tile, query_startDate, query_endDate #, map_geojson, \n",
    ")\n",
    "def get_all_results(url):\n",
    "    all_results = []\n",
    "    while url:\n",
    "        response = requests.get(url).json()\n",
    "        if 'value' in response:\n",
    "            all_results.extend(response['value'])\n",
    "        else:\n",
    "            print('Unexpected API response structure.')\n",
    "            break\n",
    "        url = response.get('@odata.nextLink')  # Move to next page if exists\n",
    "        if url:\n",
    "            time.sleep(1)  # Optional: small delay between pages\n",
    "    return all_results\n",
    "\n",
    "results = get_all_results(request_url)\n",
    "\n",
    "if not results:\n",
    "    print('No data found')\n",
    "    sys.exit()\n",
    "\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "data_id_list = df.Id\n",
    "data_name_list = df.Name\n",
    "date_content_length = df.ContentLength\n",
    "\n",
    "for i in range(len(data_id_list)):\n",
    "    print(data_name_list[i])\n",
    "    data_id = data_id_list[i]\n",
    "    data_name = get_file_name(data_name_list[i])\n",
    "    data_length = date_content_length[i]\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_file = os.path.join(output_dir, data_name)\n",
    "# Check if the file has been downloaded before or it has no recorded windrows, \n",
    "# in either case, skip it and do not download it (again). If you have a partial or \n",
    "# corrupted download, you can delete the file and re-run the script.\n",
    "# adjust to your path if necessary:\n",
    "    litterrows = pd.read_csv('../files/s2_product_unique.csv')\n",
    "    samples_set = set()\n",
    "    for name in litterrows['s2_product']:\n",
    "        product_type = name[0:11]\n",
    "        date_str = name[11:26]\n",
    "        tile = name[39:45]\n",
    "        key = product_type + '_' + date_str + '_' + tile\n",
    "        samples_set.add(key)\n",
    "\n",
    "    file_name = os.path.basename(output_file).replace('.zip', '')\n",
    "    product_type = file_name[0:11]\n",
    "    date_str = file_name[11:26]\n",
    "    tile = file_name[39:45]\n",
    "    key = product_type + '_' + date_str + '_' + tile\n",
    "\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) == data_length:\n",
    "        print(output_file + ' this file already exists')   \n",
    "    elif key not in samples_set:\n",
    "        print(output_file + ' has no recorded litter rows')\n",
    "    else:\n",
    "        access_token = get_access_token(CDSE_email, CDSE_password)\n",
    "        download_data(access_token, data_id, data_name, data_length, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c73f3",
   "metadata": {},
   "source": [
    "# Upload your batch to a new kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ae7ef",
   "metadata": {},
   "source": [
    "## edit here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476c7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Amend you project root to point to /notebooks \n",
    "## or your current directory where .kaggle/ and SAFE/ folders \n",
    "## must also be located\n",
    "\n",
    "project_root = \"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/notebooks\"\n",
    "dataset_main = Path(project_root) / \"dataset_main\"\n",
    "kaggle_json_path = Path(project_root) / \".kaggle/kaggle.json\"\n",
    "kaggle_config_dir = kaggle_json_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_folders = [\n",
    "    Path(project_root) / \"SAFE/PO_S2_19\",  # change folder name!\n",
    "#    Path(project_root) / \"SAFE/CALAB_2A\"  # add multiple if multiple batches\n",
    "]\n",
    "dataset_title = \"Litter Rows Italy\"\n",
    "dataset_id = \"sarahajbane/litter-windrows-batch-4\" # change only the number to appropriate batch #\n",
    "license_name = \"CC-BY-SA-4.0\"\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = str(kaggle_config_dir)\n",
    "os.chmod(kaggle_json_path, 0o600)\n",
    "\n",
    "dataset_main.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ff04c",
   "metadata": {},
   "source": [
    "## run as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cfaef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COPY BATCH FOLDERS INTO dataset_main ===\n",
    "for src_folder in batch_folders:\n",
    "    batch_name = src_folder.name\n",
    "    dest_folder = dataset_main / batch_name\n",
    "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for zip_file in src_folder.glob(\"*.zip\"):\n",
    "        dest_file = dest_folder / zip_file.name\n",
    "        if not dest_file.exists():\n",
    "            shutil.copy2(zip_file, dest_file)\n",
    "            print(f\"Copied: {zip_file.name} ‚Üí {batch_name}\")\n",
    "        else:\n",
    "            print(f\"Skipped (already exists): {zip_file.name} in {batch_name}\")\n",
    "\n",
    "metadata_path = dataset_main / \"dataset-metadata.json\"\n",
    "if not metadata_path.exists():\n",
    "    subprocess.run([\"kaggle\", \"datasets\", \"init\", \"-u\", str(dataset_main)], check=True)\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata[\"title\"] = dataset_title\n",
    "metadata[\"id\"] = dataset_id\n",
    "metadata[\"licenses\"] = [{\n",
    "    \"name\": license_name,\n",
    "    \"title\": \"Creative Commons Attribution Share-Alike 4.0\",\n",
    "    \"path\": \"https://creativecommons.org/licenses/by-sa/4.0/\"\n",
    "}]\n",
    "\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96ace7",
   "metadata": {},
   "source": [
    "## edit here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da82048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE OR VERSION KAGGLE DATASET ===\n",
    "def upload_or_version_kaggle_dataset(folder_path, message=\"Batch 04 zip folders\"): # enter batch number\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"create\",\n",
    "            \"-p\", str(folder_path),\n",
    "            \"--dir-mode\", \"zip\"\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        print(\"Dataset created successfully!\")\n",
    "        print(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Dataset already exists. Creating a new version...\")\n",
    "        print(e.stderr)\n",
    "        version_result = subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"version\",\n",
    "            \"-p\", str(folder_path),\n",
    "            \"-m\", message,\n",
    "            \"--dir-mode\", \"zip\"\n",
    "        ], capture_output=True, text=True)\n",
    "        print(version_result.stdout)\n",
    "        print(version_result.stderr)\n",
    "\n",
    "# === RUN ===\n",
    "upload_or_version_kaggle_dataset(dataset_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26856b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created report in /Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/notebooks/SAFE/CORSI_2A/processing_report.txt\n",
      "All tiled uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "for folder_path in batch_folders:\n",
    "    file_path = os.path.join(folder_path, 'processing_report.txt')\n",
    "    tile_count = len([\n",
    "            entry for entry in os.listdir(folder_path) \n",
    "            if os.path.isfile(os.path.join(folder_path, entry))\n",
    "        ])\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(f'Upload finished, {tile_count} tiles processed and uploaded to {dataset_id}')\n",
    "    print(f'Created report in {file_path}')\n",
    "\n",
    "print(\"All tiles uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c27b4",
   "metadata": {},
   "source": [
    "# Finished! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1bb5a",
   "metadata": {},
   "source": [
    " Once you have confirmed the upload to the kaggle dataset, which you can see with the link once completed! Please run the final code block, update the notebook and push your changes to github, or let us know on slack that you have finished the upload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35313ad",
   "metadata": {},
   "source": [
    "# If your upload gets interrupted or you want to add more images batches to the existing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f159f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b31d4e",
   "metadata": {},
   "source": [
    "__Edit version message only:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a189a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_message = \"Added PO_2A_19 zip folder\"  # edit message appropriately\n",
    "\n",
    "def version_kaggle_dataset(folder_path, message=\"Updated dataset\"):\n",
    "    result = subprocess.run([\n",
    "        \"kaggle\", \"datasets\", \"version\",\n",
    "        \"-p\", str(folder_path),\n",
    "        \"-m\", message,\n",
    "        \"--dir-mode\", \"zip\"\n",
    "    ], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Kaggle versioning failed:\")\n",
    "        print(result.stderr)\n",
    "\n",
    "# === RUN ===\n",
    "version_kaggle_dataset(dataset_main, version_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c11811",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68755f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_all_files_in_directory(directory_path):\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "# This will delete all files in the specified dataset_main subfolder\n",
    "# run this only if you have finished your upload and no longer want the files stored locally\n",
    "# if you want to delete them from the safe folder as well, change \n",
    "# Path(dataset_main) to Path(src_folder) in the line below\n",
    "# Proceed with caution\n",
    "\n",
    "directory_path = Path(dataset_main) / \"CORSI_2B\"\n",
    "delete_all_files_in_directory(directory_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omdena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
